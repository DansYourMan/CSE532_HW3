{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "This assignment deals with using `textblob` and other open-source libraries to perform NLP-based analysis on documents using Python.  **All parts should use the same three documents (as outlined in Part 1 below).  In addition to your .ipynb and/or .py files, you must submit the three documents in .txt format, as well as a report document in .txt/.pdf format that answers various questions below.  It is not necessary to submit the .csv file for Part 3, since we will be executing your code.  _Just make sure it works correctly!_**  \n",
    "\n",
    "**Part 1:**<br> Select and download three texts of your choosing that represent different media or writing formats (for example, you could choose i. a novel, movie script, and play script or ii. a short story, poem, and novel, etc.)\n",
    "**Make sure you briefly descibe your documents and explain the difference between them in a paragraph.** \n",
    "\n",
    "**Part 2:**<br>\n",
    "(a) Compute word counts for each of your documents after excluding English stop words (and optionally, performing lemmatization and/or other preprocessing that you would like to employ).<br>\n",
    "(b) Create and display a bar plot for each document that include word counts for the 25 most frequent words (after the above processing).<br>\n",
    "(c) Create and display a word cloud for each document (using a mask image of your choice) that includes only the 100 most frequent words.  Note that you'll likely want to use the approach outlined in Session 25 that utilizes the `fitwords` method, since you will want data consistent with those for part (b).<br>(d) Do you see any notable difference between the documents wrt (b) and/or (c) above?  Try to explain why or why not, and whether or not these results are expected.<br>\n",
    "\n",
    "**Part 3:**<br>\n",
    "(a) Using your approach from **Part 2**, compute the 25 most _cumulatively commmon_ words across the three documents, along with the _cumulative counts_.  Remember that a given word can appear in 2 or even all 3 documents.  <br>\n",
    "Ex: if the word \"spider\" appears 10 times in document 1, 6 times in document 2, and 5 times in document 3, its cumulative count will be 21.<br>\n",
    "(b) Create a CSV file named **MCW.csv** with the following specifications:\n",
    "i. The csv file should use the standard delimiter (,) <br> \n",
    "ii. The first row in the file should be a column header row denoted by the string \"Word,Count\" <br>\n",
    "iii. The next 25 rows should be populated with the pairs of the 25 most cumulatively common words and counts, in descending order by count. <br>\n",
    "iv. One final row should added of the form \"Sum,(totalcount)\" where (totalcount) represents the sum of the top 25 cumulative counts.<br>\n",
    "A sample csv file is included to give you an idea of what to generate in practice.<br><br>\n",
    "**Part 4:**<br>\n",
    "(a) Use **Textatistic** to compute the _average_ of the Flesch–Kincaid, Gunning Fog, SMOG, and Dale–Chall scores for each document.   \n",
    "(b) Are there noticeable differences among your documents's readability scores, and would you expect these differences (or lack of differences, if there are none) to be present among documents were you judging their readability manually?\n",
    "\n",
    "**Part 5:**<br> \n",
    "(a) Use spaCy to compute the pairwise similarity between your documents (i.e. doc. 1 to doc. 2, doc. 1 to doc. 3, doc. 2 to doc. 3).<br>\n",
    "(b) Do any of these similarity scores seem higher or lower than you would expect?  Explain your response.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1 Solution:\n",
    "For the three documents I chose to represent a novel, movie script, and a poem. For the novel I used “The Young Castellan: A Tale of the English Civil War by George Manville Fenn”, chosen because I had already used it for homework 2. Thus I had the document available and preprocessed for this assignment. The novel is a long historical piece written contemporary to the English Civil War. For the movie script I chose the Bee Movie, because txt files of the script is readably available online with special characters removed. Finally for the poem I downloaded Respite by Elane Kim, which is a short modern poem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set provided in homework 2\n",
    "stop_words = [\" \", \"\\n\", \"\\t\", \"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"A\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appreciate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"arise\", \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"B\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"been\", \"before\", \"beforehand\", \"beginnings\", \"behind\", \"below\", \"beside\", \"besides\", \"best\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"C\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"could\", \"couldn\", \"couldnt\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"D\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doing\", \"don\", \"done\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"E\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"F\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"G\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"H\", \"h2\", \"h3\", \"had\", \"hadn\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"have\", \"haven\", \"having\", \"he\", \"hed\", \"hello\", \"help\", \"hence\", \"here\", \"his\",\"her\",\"him\",\"she\",\"they\",\"them\",\"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hes\", \"hh\", \"hi\", \"hid\", \"hither\", \"hj\", \"ho\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"i\",\"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"im\", \"immediately\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"it\", \"itd\", \"its\", \"iv\", \"ix\", \"iy\", \"iz\", \"j\", \"J\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"K\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"ko\", \"l\", \"L\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"M\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"my\", \"n\", \"N\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"neither\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"O\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"otherwise\", \"ou\", \"ought\", \"our\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"P\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"Q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"R\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"S\", \"s2\", \"sa\", \"said\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"sent\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somehow\", \"somethan\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"sz\", \"t\", \"T\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"these\", \"they\", \"theyd\", \"theyre\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"U\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"using\", \"usually\", \"ut\", \"v\", \"V\", \"va\", \"various\", \"vd\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"W\", \"wa\", \"was\", \"wasn\", \"wasnt\", \"way\", \"we\", \"wed\", \"welcome\", \"well\", \"well-b\", \"went\", \"were\", \"weren\", \"werent\", \"what\", \"whatever\", \"whats\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"whom\", \"whomever\", \"whos\", \"whose\", \"why\", \"wi\", \"widely\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"would\", \"wouldn\", \"wouldnt\", \"www\", \"x\", \"X\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"Y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"your\", \"youre\", \"yours\", \"yr\", \"ys\", \"yt\", \"z\", \"Z\", \"zero\", \"zi\", \"zz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Word  Count\n",
      "14       roy    310\n",
      "29       sir    288\n",
      "24       ben    166\n",
      "89      dont    114\n",
      "43       boy    109\n",
      "6        see    102\n",
      "13    master     98\n",
      "208    cried     79\n",
      "729   mother     79\n",
      "96       man     78\n",
      "444     know     69\n",
      "406     will     68\n",
      "161   father     59\n",
      "227     want     58\n",
      "536    sword     53\n",
      "145    place     53\n",
      "929     lady     50\n",
      "215      ill     48\n",
      "258      men     47\n",
      "123     good     45\n",
      "399     time     44\n",
      "300  royland     43\n",
      "424  soldier     40\n",
      "694  himself     40\n",
      "557   pawson     39\n"
     ]
    }
   ],
   "source": [
    "#Part 2A, repeated from HW2\n",
    "import string\n",
    "novel_path = 'txt files/The Young Castellan: A Tale of the English Civil War by George Manville Fenn.txt'\n",
    "\n",
    "with open(novel_path, 'r') as file:\n",
    "    special_character_filter = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    txt = [word.translate(special_character_filter).strip().lower() for line in file for word in line.split()]\n",
    "\n",
    "import pandas as pd\n",
    "i = 0\n",
    "j = 0\n",
    "found = False\n",
    "uniqueWordCount = 0\n",
    "words = []\n",
    "count = []\n",
    "\n",
    "while(i<len(txt)):\n",
    "    j = 0\n",
    "    found = False\n",
    "    #Has this word already been seen\n",
    "    while((j < len(words)) and (found == False)):\n",
    "        if(txt[i] == words[j]):\n",
    "            count[j] = count[j] + 1\n",
    "            found = True\n",
    "        j += 1\n",
    "    #If not, create a new index for it\n",
    "    if(found == False):\n",
    "        words.append(txt[i])\n",
    "        count.append(1)\n",
    "        uniqueWordCount += 1\n",
    "    i += 1\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "word_counts = pd.DataFrame({'Word': words, 'Count': count})\n",
    "\n",
    "df_filtered = word_counts[~word_counts['Word'].isin(stop_words)]\n",
    "novel_filtered = df_filtered.sort_values(by='Count', ascending=False)\n",
    "\n",
    "#Filters out about 400 entires\n",
    "print(novel_filtered.head(25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Word  Count\n",
      "56                324\n",
      "12         bee     87\n",
      "34        dont     64\n",
      "33        bees     58\n",
      "228       know     53\n",
      "48       barry     50\n",
      "237      honey     49\n",
      "146       yeah     28\n",
      "449   thinking     27\n",
      "377        see     26\n",
      "232       life     24\n",
      "611    flowers     23\n",
      "506       want     19\n",
      "206       will     19\n",
      "347    talking     19\n",
      "236       work     19\n",
      "74        good     18\n",
      "691       time     18\n",
      "240     pollen     17\n",
      "37      humans     16\n",
      "555      roses     16\n",
      "738    vanessa     15\n",
      "1149    benson     15\n",
      "16         fly     15\n",
      "599       buzz     13\n"
     ]
    }
   ],
   "source": [
    "#Problem 2A, bee movie script\n",
    "import string\n",
    "script_path = 'txt files/Bee Movie.txt'\n",
    "\n",
    "with open(script_path, 'r') as file:\n",
    "    special_character_filter = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    txt = [word.translate(special_character_filter).strip().lower() for line in file for word in line.split()]\n",
    "\n",
    "import pandas as pd\n",
    "i = 0\n",
    "j = 0\n",
    "found = False\n",
    "uniqueWordCount = 0\n",
    "words = []\n",
    "count = []\n",
    "\n",
    "while(i<len(txt)):\n",
    "    j = 0\n",
    "    found = False\n",
    "    #Has this word already been seen\n",
    "    while((j < len(words)) and (found == False)):\n",
    "        if(txt[i] == words[j]):\n",
    "            count[j] = count[j] + 1\n",
    "            found = True\n",
    "        j += 1\n",
    "    #If not, create a new index for it\n",
    "    if(found == False):\n",
    "        words.append(txt[i])\n",
    "        count.append(1)\n",
    "        uniqueWordCount += 1\n",
    "    i += 1\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "word_counts = pd.DataFrame({'Word': words, 'Count': count})\n",
    "\n",
    "df_filtered = word_counts[~word_counts['Word'].isin(stop_words)]\n",
    "script_filtered = df_filtered.sort_values(by='Count', ascending=False)\n",
    "\n",
    "print(script_filtered.head(25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Word  Count\n",
      "0           it’s      2\n",
      "2         year’s      2\n",
      "38        bodies      2\n",
      "28      sardines      2\n",
      "57      generous      2\n",
      "96        static      1\n",
      "93        places      1\n",
      "92          list      1\n",
      "91        making      1\n",
      "89     listening      1\n",
      "88          died      1\n",
      "87           day      1\n",
      "86      birthday      1\n",
      "84       confuse      1\n",
      "82       funeral      1\n",
      "81   anniversary      1\n",
      "80          week      1\n",
      "78   supermarket      1\n",
      "76          cake      1\n",
      "75         pound      1\n",
      "74         round      1\n",
      "72       cicadas      1\n",
      "95         visit      1\n",
      "128         feed      1\n",
      "122      bellies      1\n"
     ]
    }
   ],
   "source": [
    "#Problem 2A, poem\n",
    "import string\n",
    "poem_path = 'txt files/Respite.txt'\n",
    "\n",
    "with open(poem_path, 'r') as file:\n",
    "    special_character_filter = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    txt = [word.translate(special_character_filter).strip().lower() for line in file for word in line.split()]\n",
    "\n",
    "import pandas as pd\n",
    "i = 0\n",
    "j = 0\n",
    "found = False\n",
    "uniqueWordCount = 0\n",
    "words = []\n",
    "count = []\n",
    "\n",
    "while(i<len(txt)):\n",
    "    j = 0\n",
    "    found = False\n",
    "    #Has this word already been seen\n",
    "    while((j < len(words)) and (found == False)):\n",
    "        if(txt[i] == words[j]):\n",
    "            count[j] = count[j] + 1\n",
    "            found = True\n",
    "        j += 1\n",
    "    #If not, create a new index for it\n",
    "    if(found == False):\n",
    "        words.append(txt[i])\n",
    "        count.append(1)\n",
    "        uniqueWordCount += 1\n",
    "    i += 1\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "word_counts = pd.DataFrame({'Word': words, 'Count': count})\n",
    "\n",
    "df_filtered = word_counts[~word_counts['Word'].isin(stop_words)]\n",
    "poem_filtered = df_filtered.sort_values(by='Count', ascending=False)\n",
    "\n",
    "print(poem_filtered.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
